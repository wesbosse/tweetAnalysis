{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n",
    "This notebook demonstrates the EDA process that I took when first evaluating the Tweets. It was cleaned up at the end, but contains several comments that preserve my approach. A summary of these steps can be found in the technical report notebook\n",
    "\n",
    "--- \n",
    "\n",
    "**Library Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the classics\n",
    "import scipy.sparse as sparse\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Patch\n",
    "%matplotlib inline\n",
    "\n",
    "# import some pre-written snippets that might be useful.\n",
    "from wbcustom.roc import plot as roc_plot\n",
    "#from wbcustom.notebook_analysis import notebook_analysis as notebook\n",
    "\n",
    "# some specific data manipulation tools from sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# a few models to try at first\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\n",
    "\n",
    "# model evaluation tools\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, classification_report\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading in the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'./assets/tweets.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5c369d78518f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# read in all of the tweets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./assets/tweets.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'./assets/tweets.csv' does not exist"
     ]
    }
   ],
   "source": [
    "# read in all of the tweets\n",
    "tweets = pd.read_csv('./assets/tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initial Inspection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick look at what I'm dealing with\n",
    "tweets.info()\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22,000 tweets, 4 columns in total:\n",
    " - post_id: a unique post id (not the numeric status id from twitter)\n",
    " - text: the content of the tweet\n",
    " - date_posted: tweet's time-stamp (not in datetime format)\n",
    " - questionable_content: target column\n",
    " \n",
    "We see that a few of our data points are missing their original content. Upon first inspection, I was hoping to retrieve the missing data using `python-twitter` and the post_id column, but the values are string identifiers, and not the numeric tweet ids. Next time...  \n",
    "\n",
    "Let's check the balance of our target class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = tweets['questionable_content'].value_counts()\n",
    "\n",
    "print('Counts', cb.values)\n",
    "print('Composition', cb.values/len(tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2902 tweets are marked as questionable, meaning 13%. We will need to keep this slight imbalance in mind during modeling, but for now let's take a look at the 70 tweets that are missing text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Addressing the Missing Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather tweets with null values in the text column\n",
    "null = tweets[tweets['text'].isnull()]\n",
    "\n",
    "null.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the labels of the missing tweets?\n",
    "null['questionable_content'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of them are labeled as questionable, but we can't hope to figure out why without the tweet's text or the ability to look up the original tweet itself. Dropping these tweets only removes .3% of our data, so I will go ahead and drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop tweets with any null values\n",
    "tweets = tweets.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quick Look at `date_posted`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the column to a datetime object, rename it to timestamp\n",
    "try:\n",
    "    tweets['timestamp'] = pd.to_datetime(tweets['date_posted'])    \n",
    "    tweets = tweets.drop(columns='date_posted')\n",
    "except KeyError:\n",
    "    pass\n",
    "\n",
    "# create columns with each time scale value\n",
    "timestamp = pd.DataFrame({\n",
    "    'hour': tweets['timestamp'].dt.hour,\n",
    "    'day': tweets['timestamp'].dt.day,\n",
    "    'month': tweets['timestamp'].dt.month,\n",
    "    'year': tweets['timestamp'].dt.year,\n",
    "    'day_of_week': tweets['timestamp'].dt.dayofweek,\n",
    "    'week_of_year': tweets['timestamp'].dt.weekofyear\n",
    "})\n",
    "\n",
    "# add this new info to the data\n",
    "tweets = pd.concat(objs=[tweets, timestamp], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a glance at the new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(tweets['year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some tweets are stamped with a 1970 post date. This is obviously an error as twitter wasn't created until 2006. Using the year 1970 as a mask, we can dig in to these erroneous timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[tweets['year'] == 1970].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like these years are mislabled, but the rest of the timestamp is ok. We will avoid using the year in the model, knowing there are errors. Now lets plot out the count and questionable percentage of tweets on each time scale to look for any trends that exist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in ['hour', 'day', 'month', 'year', 'day_of_week', 'week_of_year']:\n",
    "    \n",
    "    # group tweets by different specified time scale\n",
    "    groupings = tweets.groupby(by=[tweets[column]])\n",
    "\n",
    "    # create the variables to plot: tweet counts, questionable content percentages, and time intervals\n",
    "    y1 = list(groupings['questionable_content'].count())\n",
    "    y2 = list(groupings['questionable_content'].sum()*100/groupings['questionable_content'].count())\n",
    "    x = list(set(groupings.keys[0]))\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'index': x,\n",
    "        'count': y1,\n",
    "        'questionable percentage': y2\n",
    "    })\n",
    "    \n",
    "    # instantiate plot\n",
    "    width= .4\n",
    "    fig = plt.figure(figsize=(12,4)) # create figure\n",
    "    \n",
    "    # create the stacked subplots\n",
    "    ax = fig.add_subplot(111) \n",
    "    ax2 = ax.twinx() # copy for side by side bars\n",
    "\n",
    "    # use pandas to populate subplots\n",
    "    df['count'].plot(kind='bar', color='red', ax=ax, width=width, position=1)\n",
    "    df['questionable percentage'].plot(kind='bar', color='blue', ax=ax2, width=width, position=0)\n",
    "    \n",
    "    # set label and color for yticks for first plot\n",
    "    ax.set_ylabel('Tweet Count', )\n",
    "    for tl in ax.get_yticklabels():\n",
    "        tl.set_color('b')\n",
    "    \n",
    "    # again for second plot\n",
    "    ax2.set_ylabel('Percent Questionable (%)',)\n",
    "    for tl in ax2.get_yticklabels():\n",
    "        tl.set_color('r')\n",
    "        \n",
    "    # display final viz\n",
    "    plt.title(f'Tweet breakdown by {column.capitalize()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These charts need to be cleaned up (specifically left spacing and xticks for by-year), but it gives us a quick look at the different trends. If these charts were required in production, I would likely use tableau to add tooltips. \n",
    "\n",
    "Some thoughts from looking at these plots:\n",
    "- There appears to be a visual pattern in the hourly and day_of_week breakdowns. \n",
    "- There is an influx of tweets earlier in the day, with a lower proportion of questionable content being posted during that time. \n",
    "- Additionally, the percentage of questionable content has dropped off in the past two years despite a higher tweet counts.\n",
    "\n",
    "Ultimately, we would need to dive in to the data collection methods, timestamps relative to user's timezone, etc., before this information would be more useful to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(tweets.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No real correlations with different time scales and label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Baseline Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the X and y variables, split them up into training and testing sets. Split maintains the class balance in the y variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tweets['text']\n",
    "y = tweets['questionable_content']\n",
    "\n",
    "# stratify on y variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .25, stratify = y, random_state = 23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate a stock count vectorizer to build a simple model based on word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer()\n",
    "\n",
    "X_train_cvec = cvec.fit_transform(X_train)\n",
    "X_test_cvec = cvec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe out of the vectorized test data\n",
    "vec_feats = pd.DataFrame(X_test_cvec.todense(), columns = cvec.get_feature_names())\n",
    "\n",
    "# inspect\n",
    "vec_feats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will want to scrub the non-english tweets later on as well.\n",
    "\n",
    "Lets create a few different models and see how they do. Use the `%%time` magic command to get some efficiency info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rfc = RandomForestClassifier(random_state = 23, n_estimators=50)\n",
    "rfc.fit(X_train_cvec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train Score: ', rfc.score(X_train_cvec, y_train))\n",
    "print('Test Score: ', rfc.score(X_test_cvec, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 2 seconds, we get a fairly 'accurate' model, but we aren't concerned with accuracy. Since we are trying to limit false negatives, we want to maximize recall. Let's dig deeper.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the classification report function for more classification metrics\n",
    "print(classification_report(y_test, rfc.predict(X_test_cvec)))\n",
    "print('---')\n",
    "confusion_matrix(y_test, rfc.predict(X_test_cvec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall of .57 means we are only correctly questionable tweets 57% of the time. We can do better. As we see in the confusion matrix, 315 of the 725 questionable tweets in the test set were incorrectly labled as not questionable.\n",
    "\n",
    "Let's see what our model is using to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get feature importances from the model, plot\n",
    "pd.DataFrame(\n",
    "    rfc.feature_importances_, \n",
    "    index=cvec.get_feature_names()\n",
    ").sort_values(by=0, ascending=False).head(20).plot(kind='barh', figsize=(12,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The presence of common curse words and racial slurs are good indicators of a questionable tweet. Now a boosted model, to see if we can avoid the RFC's overfitting and improve sensitivity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "abc = AdaBoostClassifier(n_estimators=50)\n",
    "abc.fit(X_train_cvec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, abc.predict(X_test_cvec)))\n",
    "print('---')\n",
    "confusion_matrix(y_test, abc.predict(X_test_cvec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better accuracy, same recall, fit time was 1/10th the RandomForest's. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(abc.feature_importances_, index=cvec.get_feature_names()).sort_values(by=0, ascending=False).head(20).plot(kind='barh', figsize=(12,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar features, we will use this info for our feature engineering. We will now plot a roc curve to check if playing with our probability thresholds can help us tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom roc curve function (need to add auc score)\n",
    "roc_plot(abc.predict_proba(X_test_cvec), y_test)\n",
    "roc_auc_score(abc.predict(X_test_cvec), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This curve shows that we have a steep false positive slope, meaning we will get dramatically more false positives as we tune the probability threshold for recall (sensitivity). In our context, we don't mind more tweets to review if it means less get through the filter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a grip on what is going on, let's dive in to some [feature engineering and modeling](./Data-Prep-and-Feature-Extraction.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
